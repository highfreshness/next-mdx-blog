---
title: ğŸ§‘â€ğŸ’» KoElectra to ONNX(Pytorch)
description: Pytorch ëª¨ë¸ ONNX Runtimeìœ¼ë¡œ ë³€í™˜
date: 2024-01-01
published: true
---


# KoElectra to ONNX(Pytorch)

## í™˜ê²½

1. Docker ì»¨í…Œì´ë„ˆ
    1. Python ì´ë¯¸ì§€(latest : 3.11.4)

## Convert.py

```python
import torch
from transformers import TextClassificationPipeline, AutoTokenizer, ElectraForSequenceClassification

# ëª¨ë¸ ë¡œë”©
model = torch.load('model.pt', map_location=torch.device('cpu')) # torch.saveë¥¼ í†µí•´ ì €ì¥í•œ ëª¨ë¸ ì „ì²´ íŒŒì¼(model.pt)

# í† í¬ë‚˜ì´ì € ë¡œë”©
tokenizer = AutoTokenizer.from_pretrained("monologg/koelectra-base-v3-discriminator")

model.eval()
test_string = "ì¼ì€ ì™œ í•´ë„ í•´ë„ ëì´ ì—†ì„ê¹Œ? í™”ê°€ ë‚œë‹¤. ë§ì´ í˜ë“œì‹œê² ì–´ìš”. \
								ì£¼ìœ„ì— ì˜ë…¼í•  ìƒëŒ€ê°€ ìˆë‚˜ìš”? ê·¸ëƒ¥ ë‚´ê°€ í•´ê²°í•˜ëŠ” ê²Œ ë‚˜ì•„. \
								ë‚¨ë“¤í•œí…Œ ë¶€ë‹´ ì£¼ê³  ì‹¶ì§€ë„ ì•Šê³ . í˜¼ì í•´ê²°í•˜ê¸°ë¡œ í–ˆêµ°ìš”. \
								í˜¼ìì„œ í•´ê²°í•˜ê¸° í˜ë“¤ë©´ ì£¼ìœ„ì— ì˜ë…¼í•  ì‚¬ëŒì„ ì°¾ì•„ë³´ì„¸ìš”."
input_ids = tokenizer.encode(test_string, return_tensors='pt')

torch.onnx.export(
    model,     # model ì„¤ì •
    input_ids, # í† í¬ë‚˜ì´ì €ì˜ ì¶œë ¥ì„ inputìœ¼ë¡œ ì„¤ì •
    "monologg-koelectra-base-v3-discriminator.onnx", # model path
    export_params=True,
    opset_version=12,
    do_constant_folding=True,
    input_names = ['modelInput'],
    output_names = ['modelOutput'],
    dynamic_axes = {
        'modelInput' : {0 : 'batch_size'}, # batch_sizeì— ë”°ë¼ shape(0ë²ˆì§¸)ê°€ ë³€ë™ë  ìˆ˜ ìˆìŒ
        'modelOutput' : { 0 : 'batch_size'} # batch_sizeì— ë”°ë¼ shape(0ë²ˆì§¸)ê°€ ë³€ë™ë  ìˆ˜ ìˆìŒ
    }
)

print()
print("Model has been converted to ONNX")
```

## ONNX_test.py

```python
import onnxruntime as ort
import numpy as np
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("monologg/koelectra-base-v3-discriminator")

test_string = "ì¼ì€ ì™œ í•´ë„ í•´ë„ ëì´ ì—†ì„ê¹Œ? í™”ê°€ ë‚œë‹¤. ë§ì´ í˜ë“œì‹œê² ì–´ìš”. \
								ì£¼ìœ„ì— ì˜ë…¼í•  ìƒëŒ€ê°€ ìˆë‚˜ìš”? ê·¸ëƒ¥ ë‚´ê°€ í•´ê²°í•˜ëŠ” ê²Œ ë‚˜ì•„. \
								ë‚¨ë“¤í•œí…Œ ë¶€ë‹´ ì£¼ê³  ì‹¶ì§€ë„ ì•Šê³ . í˜¼ì í•´ê²°í•˜ê¸°ë¡œ í–ˆêµ°ìš”. \
								í˜¼ìì„œ í•´ê²°í•˜ê¸° í˜ë“¤ë©´ ì£¼ìœ„ì— ì˜ë…¼í•  ì‚¬ëŒì„ ì°¾ì•„ë³´ì„¸ìš”."

input_ids = tokenizer.encode(test_string, return_tensors='pt')
input_ids_np = input_ids.cpu().detach().numpy() # tensorë¥¼ numpy íƒ€ì…ìœ¼ë¡œ ë³€ê²½

ort_sess = ort.InferenceSession("monologg-koelectra-base-v3-discriminator.onnx")
outputs = ort_sess.run(None, {'modelInput':input_ids_np})
print(outputs)
```

## ì°¸ê³  ì‚¬ì´íŠ¸

[Pytorch ëª¨ë¸ ONNX íŒŒì¼ë¡œ ë³€í™˜ _ ëª¨ë¸ ì„œë¹™](https://moriah-blog.tistory.com/entry/Pytorch-ëª¨ë¸-ONNX-íŒŒì¼ë¡œ-ë³€í™˜-ëª¨ë¸-ì„œë¹™)